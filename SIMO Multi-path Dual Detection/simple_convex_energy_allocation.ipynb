{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random channels -> just randomly generated with variance 1\n",
    "\n",
    "# Number of samples for each channel\n",
    "num_samples = 1\n",
    "P_t = torch.tensor(21.0)\n",
    "\n",
    "# Mean and variance\n",
    "mean = 0  \n",
    "# Assuming the mean is 0 for all channels\n",
    "\n",
    "variance = torch.tensor(1, dtype = torch.float32)  \n",
    "# Variance is set to 1 for all channels\n",
    "\n",
    "# Generate the channels\n",
    "\n",
    "channels = torch.rand(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through LSTM layer\n",
    "        # x shape: (batch, seq_len, input_size), here seq_len is 1\n",
    "        lstm_out, hidden = self.lstm(x)\n",
    "\n",
    "        # Forward through linear layer\n",
    "        # Reshape the output to (batch, output_size)\n",
    "        out = self.linear(lstm_out.reshape(x.shape[0], -1))\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MyLSTM_p(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM_p, self).__init__()\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through LSTM layer\n",
    "        # x shape: (batch, seq_len, input_size), here seq_len is 1\n",
    "        lstm_out, hidden = self.lstm(x)\n",
    "\n",
    "        # Forward through linear layer\n",
    "        # Reshape the output to (batch, output_size)\n",
    "        out = self.linear(lstm_out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomActivationFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "\n",
    "        # Apply the piecewise function\n",
    "        output = torch.where(input < -1, -2 * input - 1, \n",
    "                             torch.where(input > 1, 2 * input, input**2))\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "\n",
    "        # Compute the gradient for each piece\n",
    "        grad_input = torch.where(input < -1, -2 * torch.ones_like(input),\n",
    "                                 torch.where(input > 1, 2 * torch.ones_like(input), 2 * input))\n",
    "        return grad_input * grad_output\n",
    "\n",
    "# To apply the custom activation function\n",
    "def projection(input):\n",
    "    return CustomActivationFunction.apply(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrangian_eq(\n",
    "    h, \n",
    "    p, \n",
    "    P_t,\n",
    "    lambda_,\n",
    "    variance = 1,\n",
    "):\n",
    "    '''### - First calculate the lagrangian equation.\n",
    "\n",
    "    ### - Then do backward:\n",
    "     - calculate the gradient for both subsequently in two networks\n",
    "     - grad P | grad Lambda are the return of the function\n",
    "    '''\n",
    "    \n",
    "    _lam = projection(lambda_)\n",
    "    _L = torch.log2(1 + torch.dot(h, p)) - _lam * (torch.sum(p) - P_t)\n",
    "    # do backward, calculate gradient\n",
    "    _L.backward()\n",
    "\n",
    "    return p.grad, lambda_.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation of the problem\n",
    "\n",
    "lstm_lambda = MyLSTM(1, 20, 1)\n",
    "lstm_x = MyLSTM_p(6, 20, 6)\n",
    "optimiser_lambda = torch.optim.Adam(\n",
    "    lstm_lambda.parameters(),\n",
    "    lr = 1e-3\n",
    ")\n",
    "\n",
    "optimiser_p = torch.optim.Adam(\n",
    "    lstm_x.parameters(),\n",
    "    lr = 1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.9296], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/opt/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/ds/lnt43mls129cb8n4b_0_fkhr0000gn/T/ipykernel_25147/2363290861.py\", line 27, in <module>\n",
      "    _P_iteration = lstm_x(P_grad.reshape(1,1,6)).squeeze()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/var/folders/ds/lnt43mls129cb8n4b_0_fkhr0000gn/T/ipykernel_25147/327897627.py\", line 38, in forward\n",
      "    lstm_out, hidden = self.lstm(x)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py\", line 769, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 6]] is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ds/lnt43mls129cb8n4b_0_fkhr0000gn/T/ipykernel_25147/2363290861.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     ) - _lambda_next * (torch.sum(_p_next) - P_t)\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0moptimiser_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 6]] is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "# initialise the parameters\n",
    "_p = torch.rand(6, requires_grad = True)\n",
    "_lambda = torch.rand(1, requires_grad=True)\n",
    "# Enable anomaly detection\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range (100):\n",
    "\n",
    "    # here _p is the proposed optimised variable, and will iterate with _lambda\n",
    "    P_grad, Lambda_grad = lagrangian_eq(channels, _p, P_t, _lambda)\n",
    "\n",
    "    # first iterate lambda\n",
    "    _lambda_iteration = lstm_lambda(Lambda_grad.reshape(1,1,1)).squeeze()\n",
    "    _lambda_next = projection(_lambda + _lambda_iteration)\n",
    "\n",
    "    loss_lambda = torch.log2(\n",
    "        1 + torch.dot(channels, _p)\n",
    "    ) - _lambda_next * (torch.sum(_p) - P_t)\n",
    "    \n",
    "    print (loss_lambda)\n",
    "\n",
    "    loss_lambda.backward()\n",
    "    optimiser_lambda.step()\n",
    "\n",
    "    _lambda = projection(_lambda)\n",
    "    P_grad, Lambda_grad = lagrangian_eq(channels, _p, P_t, _lambda)\n",
    "    _P_iteration = lstm_x(P_grad.reshape(1,1,6)).squeeze()\n",
    "    _p_next = _p + _P_iteration\n",
    "    loss_p = - torch.log2(\n",
    "        1 + torch.dot(channels, _p_next)\n",
    "    ) - _lambda_next * (torch.sum(_p_next) - P_t)\n",
    "\n",
    "    loss_p.backward()\n",
    "    optimiser_p.step()\n",
    "\n",
    "    _p = _p_next.clone()\n",
    "    _lambda = _lambda_next.clone()\n",
    "\n",
    "print (_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal power allocation is:\n",
      "[1.27158871e-09 5.66728855e-01 1.86959056e-02 6.44981527e-08\n",
      " 2.09707023e-08 4.14575151e-01]\n"
     ]
    }
   ],
   "source": [
    "p = cp.Variable(6)\n",
    "\n",
    "# Define the objective function\n",
    "objective = cp.Maximize(\n",
    "    cp.sum(cp.log1p(cp.multiply(channels.squeeze(), p))) / np.log(2)\n",
    ")\n",
    "\n",
    "\n",
    "# Define the constraints\n",
    "constraints = [p >= 0, cp.sum(p) <= 1]\n",
    "\n",
    "# Define the problem and solve it\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve(solver=cp.ECOS)\n",
    "\n",
    "# Print the optimal power allocation\n",
    "print(\"The optimal power allocation is:\")\n",
    "print(p.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
